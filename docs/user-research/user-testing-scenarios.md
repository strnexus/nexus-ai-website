# User Testing Scenarios & Questions for SMB Owners

## Testing Overview
**Objective**: Validate user journey assumptions and identify friction points through real SMB owner feedback
**Method**: Remote moderated user testing sessions (30-45 minutes each)
**Participants**: 3 SMB owners from different industries and business stages
**Tools**: Screen recording + think-aloud protocol

---

## PARTICIPANT SELECTION CRITERIA

### Required Qualifications:
- **Business Size**: 1-50 employees
- **Role**: Owner, founder, or decision-maker for technology purchases
- **Industry**: Diverse mix (service-based, retail, professional services)
- **Tech Comfort**: Mixed levels (at least one less tech-savvy participant)
- **AI Experience**: Varied (complete beginner to some exposure)

### Target Participant Mix:
1. **Participant A**: Service business owner (plumber, electrician, contractor)
2. **Participant B**: Professional services (accountant, lawyer, consultant)  
3. **Participant C**: Retail/e-commerce small business owner

---

## TESTING SCENARIOS

### Scenario 1: "The Urgent Problem Solver"
**Setup**: *"Imagine you're a [participant's business type] owner. You've been hearing about AI tools that could help businesses like yours, but you're overwhelmed by all the options out there. You have a specific challenge: [relevant business challenge]. You just Googled 'AI tools for small business' and found this website. Your goal is to figure out if this platform can actually help you solve your problem."*

**Expected Duration**: 15-20 minutes
**Key Focus**: First impressions, value proposition clarity, tool discovery process

**Observation Points**:
- Initial reaction to homepage
- Navigation pattern (directory vs Nova vs agency)
- Tool relevance assessment
- Decision points and hesitations

### Scenario 2: "The Cautious Researcher"
**Setup**: *"You've been burned by overhyped business tools before. You're interested in AI but want to make sure you're not wasting time or money. You've heard good things about this platform from a business associate. Your goal is to thoroughly evaluate whether this is worth your time and if the recommendations are trustworthy."*

**Expected Duration**: 20-25 minutes
**Key Focus**: Trust-building, information verification, skeptical user behavior

**Observation Points**:
- Trust indicators that resonate
- Information seeking behavior
- Comparison with existing knowledge
- Proof requirements

### Scenario 3: "The Implementation Ready"
**Setup**: *"Your business is doing well, but you know you need to modernize to stay competitive. You've decided to invest in professional help implementing AI tools. You're comparing different agencies and consultants. Your goal is to determine if this agency is the right fit for your needs and budget."*

**Expected Duration**: 15-20 minutes
**Key Focus**: Agency evaluation process, consultation booking flow, credibility assessment

**Observation Points**:
- Credibility indicators
- Service differentiation
- Booking process ease
- Price sensitivity reactions

---

## TESTING PROTOCOL

### Pre-Session Setup (5 minutes)
1. **Warm-up Questions**:
   - "Tell me about your business and your role"
   - "What's your biggest operational challenge right now?"
   - "Have you used any AI tools before? What was that experience like?"
   - "How do you usually research new business tools or services?"

### Session Structure (30-40 minutes)
1. **Scenario Introduction** (2-3 minutes)
2. **Guided Exploration** (15-20 minutes per scenario)
3. **Post-scenario Questions** (5-8 minutes)
4. **Wrap-up Discussion** (5-10 minutes)

### Think-Aloud Instructions:
*"I'd like you to think out loud as you explore. Tell me what you're looking at, what you're thinking, what questions come to mind, and what you're trying to accomplish. There are no wrong answers - I'm interested in understanding your natural thought process."*

---

## KEY TESTING QUESTIONS

### Homepage First Impressions (0-30 seconds)
1. **"What do you think this website is for?"**
   - Looking for: Value proposition clarity
   - Success indicator: Mentions AI tools directory AND agency services

2. **"Who do you think this is designed for?"** 
   - Looking for: Target audience recognition
   - Success indicator: "Small business owners like me"

3. **"What would you click first and why?"**
   - Looking for: Clear next steps, intuitive navigation
   - Success indicator: Confident choice with clear reasoning

4. **"On a scale of 1-10, how professional and trustworthy does this look?"**
   - Looking for: Initial trust assessment
   - Success indicator: 7+ with specific reasons

### Tool Discovery Process (2-8 minutes)
5. **"How would you find tools relevant to your business?"**
   - Looking for: Discovery method preferences
   - Success indicator: Successful use of filters or Nova

6. **"What information would you need to feel confident recommending a tool to another business owner?"**
   - Looking for: Trust and credibility requirements
   - Success indicator: Pricing, use cases, integration info

7. **"If you found a tool that looked promising, what would your next step be?"**
   - Looking for: Research behavior patterns
   - Success indicator: Clear progression path

### Nova Interaction (3-10 minutes)
8. **"What's your first impression of this AI assistant?"**
   - Looking for: Personality and helpfulness perception
   - Success indicator: "Helpful" and "business-focused"

9. **"How would you test whether this AI actually understands your business needs?"**
   - Looking for: Validation strategies
   - Success indicator: Specific business questions

10. **"Would you prefer to browse on your own or get guided recommendations? Why?"**
    - Looking for: Interaction preferences
    - Success indicator: Clear preference with reasoning

### Value Recognition (5-15 minutes)
11. **"What would convince you that this platform is different from other AI tool websites you might have seen?"**
    - Looking for: Differentiation factors
    - Success indicator: Mentions personalization, SMB focus, or expert guidance

12. **"If you were explaining this to your business partner, how would you describe the value?"**
    - Looking for: Value articulation
    - Success indicator: Clear business benefit explanation

### Conversion Consideration (10-20 minutes)
13. **"At what point would you be willing to provide your email address?"**
    - Looking for: Soft conversion triggers
    - Success indicator: Clear value exchange understanding

14. **"What would need to happen for you to feel comfortable paying for this service?"**
    - Looking for: Purchase decision factors
    - Success indicator: ROI justification or risk mitigation needs

15. **"If you needed expert help implementing AI tools, what would make you choose this agency over others?"**
    - Looking for: Agency selection criteria
    - Success indicator: SMB expertise and process clarity

### Mobile Experience (If applicable)
16. **"How does this compare to using it on your phone versus desktop?"**
    - Looking for: Mobile usability assessment
    - Success indicator: Functional parity or clear mobile advantages

### Overall Assessment
17. **"What's the one thing that would most improve this experience?"**
    - Looking for: Primary friction point
    - Success indicator: Actionable, specific feedback

18. **"Would you bookmark this site? Why or why not?"**
    - Looking for: Return intent and value perception
    - Success indicator: Clear return use case

19. **"On a scale of 1-10, how likely would you be to recommend this to another business owner?"**
    - Looking for: Net Promoter Score indicator
    - Success indicator: 7+ with specific reasons

---

## OBSERVATION FRAMEWORK

### Behavioral Indicators to Track:

#### Positive Engagement Signals:
- âœ… Scrolls below the fold
- âœ… Uses filtering/search features
- âœ… Clicks into tool details
- âœ… Engages with Nova for multiple exchanges
- âœ… Spends time reading descriptions
- âœ… Asks clarifying questions about features

#### Friction/Confusion Signals:
- âŒ Quick back-button usage
- âŒ Repetitive clicking without progress
- âŒ Long pauses with uncertain expressions
- âŒ Mentions of confusion or overwhelm
- âŒ Comparisons to "every other" similar site
- âŒ Questions about basic functionality

#### Decision-Making Patterns:
- ðŸ¤” Price-checking behavior early vs late
- ðŸ¤” Social proof seeking (reviews, testimonials)
- ðŸ¤” Feature comparison approaches
- ðŸ¤” Risk assessment questions
- ðŸ¤” Implementation concern expressions

---

## SUCCESS CRITERIA

### Session-Level Success Indicators:
1. **Comprehension**: Participant understands dual value proposition (directory + agency)
2. **Engagement**: Uses both tool discovery and Nova features
3. **Relevance**: Finds tools applicable to their business type
4. **Trust**: Expresses confidence in information quality
5. **Progression**: Shows clear path toward conversion action

### Overall Testing Success Metrics:
- **2/3 participants** successfully complete primary task flow
- **100% comprehension** of platform purpose within 60 seconds
- **Average engagement time** >15 minutes per scenario
- **Trust rating** 7+ from majority of participants
- **Clear friction identification** in <5 key areas

---

## POST-SESSION ANALYSIS

### Data Collection:
1. **Screen recordings** with timestamps for key moments
2. **Verbatim quotes** for critical insights
3. **Behavioral pattern notes** for each user type
4. **Pain point categorization** by severity and frequency
5. **Feature preference rankings** across participants

### Analysis Framework:
- **Friction Severity**: High (blocks progression), Medium (slows progression), Low (minor annoyance)
- **Pattern Frequency**: Consistent (all participants), Common (2/3), Individual (1/3)
- **Fix Complexity**: Quick wins, Medium effort, Major redesign needed
- **Business Impact**: Revenue-critical, Engagement-critical, Nice-to-have

---

## TESTING LOGISTICS

### Session Scheduling:
- **Duration**: 45 minutes (30 minutes testing + 15 minutes buffer)
- **Timing**: Business hours, participant's timezone
- **Platform**: Zoom with screen share capability
- **Recording**: Both video and audio with participant consent

### Compensation:
- **Payment**: $100 gift card per participant
- **Thank you**: Follow-up with key findings summary

### Team Roles:
- **Moderator**: Guides session, asks questions
- **Observer**: Takes detailed notes, tracks behaviors
- **Tech Support**: Handles recording, technical issues

This testing framework will provide concrete validation of our user journey assumptions and actionable insights for UX optimization before moving to the development phase.